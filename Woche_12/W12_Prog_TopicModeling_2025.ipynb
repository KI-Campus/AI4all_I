{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3548d6df6b60e2",
   "metadata": {},
   "source": [
    "# Topic Modeling mit scikit-learn\n",
    "\n",
    "Du kennst jetzt schon die Funktionsweise von Topic Modeling. In diesem Notebook schauen wir uns jetzt einmal an, wie du mithilfe der Clusteringmethode KMeans aus einem Korpus die relevanten Themen extrahieren kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1592ae7f53372c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:29.464240Z",
     "start_time": "2025-03-17T11:59:29.451569Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61de82ba0577723",
   "metadata": {},
   "source": [
    "## Korpus\n",
    "\n",
    "\n",
    "Um Themen zu extrahieren, benötigen wir zuerst ein Korpus. Da trifft es sich doch gut, dass im Python-Modul scikit-learn bereits ein Textkorpus zur Verfügung gestellt wird. Es handelt sich hier um den Datensatz `20newsgroups`, in dem englischsprachige Beiträge aus 20 verschiedenen Online-Foren der 90er Jahre gesammelt wurden. Diese decken ein breites Spektrum an Diskussionsthemen ab, z. B. Sport, Politik und Wissenschaft. Wir laden den Datensatz mit dem Befehl `fetch_20newsgroups` aus dem Modul `sklearn.datasets`. Bei der Funktion können wir mit `subset=''` bereits angeben, welche Foren wir betrachten wollen, in diesem Fall alle.\n",
    "\n",
    "Mit dem Datensatz kann auch eine Klassifikation durchgeführt werden, bei denen die Texte zu den speziellen Foren zugeteilt werden sollen. Daher existiert im Datensatz eine Zielvariable, die wir aber nicht benötigen. Die Option `return_X_y=True` gibt uns die Daten getrennt nach Beobachtungen und Zielvariable zurück, und der Unterstrich `_` gibt an, dass die Zielvariable nicht gespeichert werden muss. Unsere Textdokumente sind jetzt also in `documents` gespeichert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8405737-e889-43df-aa68-41bdfd6e54c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:29.742180Z",
     "start_time": "2025-03-17T11:59:29.470318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Laden des Datensatzes\n",
    "documents, _ = fetch_20newsgroups(subset='all', return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c52056091abff4f",
   "metadata": {},
   "source": [
    "Eine Beispielnachricht sieht dann so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d760e-44cb-48d2-afb2-287aba48f20e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:29.758715Z",
     "start_time": "2025-03-17T11:59:29.755388Z"
    }
   },
   "outputs": [],
   "source": [
    "#Gebe den Beitrag Nummer 30 aus\n",
    "print(documents[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e1ccf223622",
   "metadata": {},
   "source": [
    "Der obere Teil ist die Kopfzeile. Hier findest du unter anderem den Benutzernamen des oder der Beitrag-Ersteller:in. Danach kommt der Textkörper. An den spitzen Klammern nach links erkennst du Zitate, also Textteile, die in anderen Beiträgen vorkommen und auf die sich der aktuelle Beitrag bezieht. Den Abschluss macht eine Fußzeile, in diesem Fall eine Signatur. Da es uns nur um die Themen der Beiträge selber geht, sollten wir auch alle Kopfzeilen, Fußzeilen und Zitate aus den Beiträgen entfernen. Wir laden die Daten also noch einmal, entfernen dieses Mal aber mit der `remove`-Option der `fetch_20newsgroups`-Methode die ungewünschten Nachrichtenbestandteile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5366d984d2cc27c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:31.232813Z",
     "start_time": "2025-03-17T11:59:29.766106Z"
    }
   },
   "outputs": [],
   "source": [
    "#Noch mal laden, dieses Mal ohne headers, footers und quotes\n",
    "documents, _ = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'), return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6fdc6d0c8a39c",
   "metadata": {},
   "source": [
    "## Bereinigung und Tf-idf-Vectorizer\n",
    "\n",
    "Jetzt haben wir ein Korpus an Dokumenten, mit dem wir aber in seiner jetzigen Form noch nichts anfangen können. Um die Themen der Dokumente erkennen zu können, brauchen wir eine geeignete Darstellung der Dokumente. Dafür können wir den TF-IDF-Vectorizer aus dem Modul scikit-learn verwenden.\n",
    "\n",
    "Der Vectorizer erstellt, wie der Name bereits vermuten lässt, aus den jeweiligen Wörtern eines Dokuments einen Vektor mit den entsprechenden tf-idf-Werten. Die Vektoren der einzelnen Dokumente werden dann zu einer Matrix zusammengefasst. Zur Erinnerung: Tf-idf steht für “term frequency – inverse document frequency” und ist ein Maß für die Relevanz eines Wortes innerhalb eines Korpus. Je häufiger ein Wort in einem Dokument und je seltener dieses Wort im gesamten Korpus auftaucht, desto höher ist der tf-idf-Wert dieses Wortes im betrachteten Dokument. So können wir auf einen Blick die relevantesten Wörter der jeweiligen Dokumente erkennen.\n",
    "\n",
    "Als Erstes sollten wir unseren Datensatz aber bereinigen. Zum Glück sind im Vectorizer bereits viele Vorverarbeitungsschritte integriert. Mit dem Parameter `vocabulary` können wir ein Wörterbuch übergeben. Mit `vocabulary=None` weisen wir den Vectorizer an, sich sein eigenes Wörterbuch zu erstellen. Wir erstellen unsere spätere tf-idf-Matrix nur mit den Wörtern des Wörterbuchs.\n",
    "\n",
    "Jetzt müssen unsere Texte noch tokenisiert werden. Durch `analyzer=’word’` geben wir an, dass unsere Tokens aus ganzen Wörtern bestehen sollen. Der Parameter `token_pattern` gibt an, wie Wörter aussehen müssen, damit sie als Token gezählt werden. Der Buchstabensalat dahinter ist ein sogenannter regulärer Ausdruck. Keine Sorge, du musst als Anfänger:in nicht wissen, was das ist, oder selber reguläre Ausdrücke lesen und schreiben können. Wenn du dich allerdings näher mit Textverarbeitung beschäftigen möchtest, sind reguläre Ausdrücke ein nützliches Hilfsmittel. Der von mir benutzte reguläre Ausdruck gibt vereinfacht gesagt an, dass nur Wörter aus mindestens zwei Buchstaben als Token betrachtet werden sollen. So werden also u. a. Ziffern und Sonderzeichen aussortiert.\n",
    "\n",
    "Als Nächstes wandeln wir mit `lowercase=True` alle Zeichen unseres Textes in Kleinbuchstaben um. Zur Erinnerung: Unsere Texte sind Strings, und alle Zeichen eines Textes sind mit ASCII kodiert. Großbuchstaben haben einen anderen ASCII-Code als die dazugehörigen Kleinbuchstaben. Nachdem unsere Texte in kleingeschriebene Wörter aufgeteilt sind, können wir die Stoppwörter entfernen. Durch `stopwords=’english’` wird eine integrierte englische Liste verwendet. Das Entfernen von zu vielen, zu wenigen oder falschen Wörtern kann aber die Ergebnisse deiner Analysen stark beeinflussen, also solltest du dir immer gut überlegen, welche Stoppwörter für deinen Kontext gut geeignet sind! Durch den Parameter `stopwords` kannst du auch eine eigene Liste an den Vectorizer übergeben.\n",
    "\n",
    "Wir erstellen also den Vectorizer durch den Befehl `TfidfVectorizer` mit den eben genannten Parameteroptionen. Wir wenden ihn an, indem wir die Methode `fit_transform` mit unserem Korpus aufrufen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd25bfbe586510",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:32.811850Z",
     "start_time": "2025-03-17T11:59:31.239992Z"
    }
   },
   "outputs": [],
   "source": [
    "#in Kleinbuchstaben umwandeln, Stopwörter entfernen, Tokenisierung per Wort, Sonderzeichen etc entfernen\n",
    "vectorizer = TfidfVectorizer(vocabulary=None, analyzer='word', token_pattern=r\"(?u)\\b[a-zA-Z]{2,}\\b\",  lowercase=True, stop_words='english' )\n",
    "\n",
    "#unseren Korpus in die gewünschte Form umwandeln\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dffdc67adf0047",
   "metadata": {},
   "source": [
    "## Themen extrahieren\n",
    "\n",
    "Da wir jetzt ein Korpus in geeigneter Darstellung haben, können wir endlich unsere Themen extrahieren. Eine Möglichkeit, dies zu tun, ist, mit dem Clustering-Algorithmus KMeans. KMeans nimmt die tf-idf-Vektoren der Dokumente, um ähnliche Dokumente zu clustern, sprich zu gruppieren. Jedes Dokumentencluster entspricht dann einem Oberthema. Die Clusteranzahl `k` ist dabei ein Maß, wie differenziert die gefundenen Themen dabei sein sollen. So werden sich bei einem kleinen `k` wahrscheinlich alle sportbezogenen Beiträge dasselbe Cluster teilen, während bei einem sehr großen `k` womöglich Unterschiede zwischen den jeweiligen Sportarten gemacht werden. In unserem Beispiel nennen wir die Clusteranzahl `topic_number`, setzen sie auf 15 und erstellen und trainieren unser Modell. Aber probiere hier gerne einmal herum!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43062a99f3cc37e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:32.830319Z",
     "start_time": "2025-03-17T11:59:32.827957Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unterschiedliche Anzahl Cluster ausprobieren\n",
    "topic_number = 15\n",
    "km = KMeans(n_clusters=topic_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05317de6-57e3-4d8a-ac94-8bd60fd94d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:33.462157Z",
     "start_time": "2025-03-17T11:59:32.848912Z"
    }
   },
   "outputs": [],
   "source": [
    "#Modell trainieren\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83016027adcd37",
   "metadata": {},
   "source": [
    "Nachdem die Cluster erstellt sind, möchten wir feststellen, welche Themen unser Modell denn identifiziert hat. Das geht natürlich nicht mit Worten, sprich KMeans gibt nicht Begriffe wie „Sport” oder „Politik” zurück. Wir behelfen uns also, indem wir uns die wichtigsten Wörter eines Dokumentenclusters ausgeben lassen. Durch den Befehl `get_feature_names_out()` erhalten wir die Features unserer Dokumente. Das sind hier die Wörter des Wörterbuches, insgesamt stolze 86.567. Außerdem definieren wir uns eine Variable `top_n`, die angibt, wie viele Wörter pro Cluster wir uns ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c650a46-89a9-4b7a-a1b7-5617d2fdf43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:33.523451Z",
     "start_time": "2025-03-17T11:59:33.472838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extrahiere für jedes Cluster die Top-Terme basierend auf den Clusterzentren\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "#Lass dir die Größe des Wörterbuches anzeigen\n",
    "print(terms.shape)\n",
    "# Anzahl der Top-Terme pro Cluster, die wir später ausgegeben bekommen möchten\n",
    "top_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3f09aa9517ee0",
   "metadata": {},
   "source": [
    " ### Optional: Erklärung der Hilfsfunktion\n",
    "\n",
    "Wir definieren uns eine Methode zum Ausgeben der Top-Terme. Du kannst eine eigene Methode definieren, indem du den Befehl `def` verwendest und danach den gewünschten Namen und die Parameter der Methode angibst. Unsere Methode soll `print_top_terms` heißen und als Parameter einmal die Terme aus dem Wörterbuch erhalten (`terms`), dann die Zentroide der gefundenen Cluster, sprich ihre Mittelpunkte (`unsorted_centroids`) und als letztes die Anzahl der Cluster (`topic_number`). Das sind nur die gewählten Namen im Rahmen der Methode, wir hätten diese Parameter also auch `x`, `y` und `z` nennen können.\n",
    "\n",
    "Damit wir die wichtigsten Terme der einzelnen Cluster bestimmen können, müssen wir uns die Terme angucken, die die höchste Gewichtung in jedem Cluster haben. Dafür müssen wir erst einmal verstehen, was ein Cluster-Mittelpunkt in unserem Fall überhaupt aussagt.\n",
    "\n",
    "\n",
    "Angenommen, wir haben das Wörterbuch `[Katze, Kaffee, Auto, Puppe, Berg]`, drei Cluster und wollen das jeweils wichtigste Wort der Dokumente im Cluster herausfinden (`top_n = 1`). Dann können die Clustermittelpunkte zum Beispiel so aussehen:\n",
    "\n",
    "```\n",
    "[[0.1, 0.5, 0.3, 0.2, 0.7],  # Cluster 1\n",
    " [0.3, 0.2, 0.8, 0.5, 0.1],  # Cluster 2\n",
    " [0.6, 0.4, 0.2, 0.1, 0.3]]  # Cluster 3\n",
    "```\n",
    "\n",
    "Bei unserem Textbeispiel gilt also: Es gibt eine Zeile für jedes Cluster und eine Spalte für jedes Wort des Wörterbuches. Die Zahlen geben die tf-idf-Scores der Wörterbuchworte des Mittelpunktes an. Je höher also der numerische Wert bei einem Wort, desto wichtiger ist dieses Wort (Feature) für das jeweilige Cluster. Damit wir die wichtigsten Wörter ausgeben können, können wir unsere Mittelpunke also sortieren. Wenn wir jetzt nur mit der `sort`-Methode die Werte sortieren, verlieren wir aber die Zuordnung: Aus `[0.1, 0.5, 0.3, 0.2, 0.7]` würde `[0.1, 0.2, 0.3, 0.5, 0.7]` werden und wir wüssten nicht mehr, welches Wort denn jetzt 0.5 beigetragen hat. Stattdessen verwenden wir die Methode `argsort`. Diese Methode gibt nicht die Werte, sondern die Indizes der Elemente in aufsteigender Reihenfolge zurück. Für den Zentroid `[0.1, 0.5, 0.3, 0.2, 0.7]` gibt `argsort` folgendes zurück: `[0, 3, 2, 1, 4]`.\n",
    "\n",
    "Am wenigsten wichtig war also das Feature an Stelle 0 (`Katze`, denk daran, dass wir mit 0 anfangen zu zählen!), am zweitwichtigsten das an Stelle 3 (`Puppe`) usw..\n",
    "\n",
    "Für unsere Beispiel-Cluster ergibt sich mit `argsort` also:\n",
    "\n",
    "```\n",
    "[[0, 3, 2, 1, 4],  # Cluster 1\n",
    " [4, 1, 0, 3, 2],  # Cluster 2\n",
    " [3, 2, 4, 1, 0]]  # Cluster 3`\n",
    "```\n",
    "\n",
    "\n",
    "Als nächstes sammeln wir unsere gefundenen Top-Terme in einer Liste. Um alle Cluster durchzugehen, benutzen wir eine sogenannte `for`-Schleife. Eine Schleife wiederholt unsere Anweisungen solange, bis eine bestimmte Bedingung erfüllt ist. Für unsere `for`-Schleife haben wir eine Zählvariable, hier `i`, die von 0 anfängt und nach jedem Schleifendurchlauf um eins erhöht wird. Bevor die Anweisungen der Schleife erneut ausgeführt werden, wird geprüft, ob die Zählvariable die Abbruchhöhe erreicht hat, hier `topic_number`, also die Anzahl an Clustern. Wenn das nicht der Fall ist, werden die Anweisungen der Schleife erneut ausgeführt, ansonsten wird das Programm mit den Anweisungen nach der Schleife weitergeführt. Du erkennst, welche Anweisungen zur Schleife gehören, weil die entsprechenden Programmzeilen eingerückt sind.\n",
    "\n",
    "Zusammengefasst bedeutet `for i in range(topic_number)` also, dass die nachfolgenden Anweisungen `topic_number`-oft durchgeführt werden sollen.\n",
    "\n",
    "Aber was genau tun wir in dieser Schleife eigentlich? In jedem Schleifendurchlauf steht `i` für den gerade betrachteten sortierten Zentroiden. Mit `sorted_centroids[i, ::-1][:top_n]]` wählen wir die hintersten `top_n` Einträge von Zentroid `i` aus. Zur Erinnerung: In den sortierten Zentroiden stehen die Indizes der Top-Terme ganz hinten, weil sie von unwichtig nach wichtig sortiert wurden.\n",
    "Falls dir die Schreibweise mit den Doppelpunkten nichts (mehr) sagt, schau dir doch mal einen Beitrag zu Slicing in Python an, zum Beispiel den hier: https://statistikguru.de/python/slicing-in-python.html\n",
    "\n",
    "Für jeden Index aus den hinteren `top_n`-Indizes (`for index in sorted_centroids[i, ::-1][:top_n]`) sucht das Programm jetzt heraus, welches Wort des Wörterbuches zu diesem Index gehört (`terms[index]`). Die eckigen Klammern um den gesamten Ausdruck zeigen an, dass diese Wörter in eine Liste geschrieben werden, die dann unter `top_terms_for_cluster` abgespeichert wird. Danach wird diese Liste an unsere `top_terms`-Liste angehängt (`top_terms.append(top_terms_for_cluster)`) und der nächste Schleifendurchlauf startet. Dabei wird die Liste `top_terms_for_cluster` mit den Top-Termen des nächsten Clusters überschrieben, sodass wir immer nur die Top-Terme des aktuell betrachteten Clusters anhängen. In unserem Beispiel wird also im ersten Schleifendurchlauf `Berg` in die Liste der Top-Terme des ersten Clusters aufgenommen, dann `Auto` für das zweite Cluster und `Katze` für das dritte Cluster, weil jeweils Index 4, 2 bzw. 0 als letzter Eintrag in den sortierten Clustermittelpunkten standen.\n",
    "\n",
    "In der nächsten `for`-Schleife (`for i, terms_for_cluster in enumerate(top_terms)`) gehen wir jetzt alle Cluster und alle Wörter aus der Liste `top_terms` durch und geben sie aus. Der Befehl `', '.join(terms_for_cluster)` verknüpft dabei alle Terme aus dem jeweiligen Listeneintrag eines Clusters mit einem Komma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa2a78f-c9d6-46b9-95db-25afd2038dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:33.542902Z",
     "start_time": "2025-03-17T11:59:33.538885Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hilfsfunktion, muss nicht verstanden werden\n",
    "def print_top_terms(terms, unsorted_centroids,topic_number):\n",
    "    # Sortieren der Clusterzentren nach Gewicht\n",
    "    sorted_centroids = unsorted_centroids.argsort()\n",
    "    top_terms = []\n",
    "    for i in range(topic_number):\n",
    "        top_terms_for_cluster = [terms[index] for index in sorted_centroids[i, ::-1][:top_n]]\n",
    "        top_terms.append(top_terms_for_cluster)\n",
    "\n",
    "    for i, terms_for_cluster in enumerate(top_terms):\n",
    "        print(\"Top-Terme für Cluster \" + str(i+1) + \": \" + ', '.join(terms_for_cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bae445fbd62f82",
   "metadata": {},
   "source": [
    "## Weiter geht's!\n",
    "\n",
    "Für das Ausgeben der Wörter nutzen wir die selbstgeschriebene Methode print_top_terms, die als Parameter die Wörterbuch-Wörter, die Clusterzentren und die Anzahl an Themen/Cluster erhält und dann die jeweiligen wichtigsten Wörter ausgibt. Die Befehle der Hilfsmethode müssen nicht von dir verstanden werden, nur die grobe Funktionsweise.\n",
    "\n",
    "Die Clusterzentren sind in der Variable `cluster_centers_` des KMeans-Modell `km` gespeichert, daher kann mit `km.cluster_centers_` darauf zugegriffen werden. Es handelt sich hier um eine Matrix, bei der jede Zeile den Mittelpunkt des jeweiligen Clusters angibt und die Spalten die Koordinaten im Raum sind. Wir haben 15 Cluster und 86567 Wörter im Wörterbuch, also einen 86567-dimensionalen Raum, in dem sich die Cluster befinden. Da wir die Dokumente in tf-idf-Vektoren umgewandelt haben, können wir durch die Koordinaten der Cluster-Mittelpunkte feststellen, welche Wörter für jedes Cluster am wichtigsten sind. Diese Wörter erkennen wir daran, dass ihre tf-idf-Werte im angegebenen Mittelpunkt am höchsten sind.\n",
    "\n",
    "Und voila: Folgende Cluster mit den jeweiligen wichtigsten Wörtern hat unser Modell gefunden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ca83f-ed4b-4fc3-a9c0-2044c5144ae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:33.584460Z",
     "start_time": "2025-03-17T11:59:33.552364Z"
    }
   },
   "outputs": [],
   "source": [
    "#Ausgabe der top-Terme mithilfe der zuvor definierten Hilfsfunktion\n",
    "print_top_terms(terms, km.cluster_centers_, topic_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf36d805288e1e9",
   "metadata": {},
   "source": [
    "Wie genau die Cluster aufgebaut sind, ändert sich natürlich von Training zu Training, daher können sich deine Begriffe von denen im Video unterscheiden. Wahrscheinlich beschäftigt sich aber mindestens ein Cluster wohl mit Computern, eins mit Sport, eins mit Weltraum, eins mit Religion, und eins mit Hilfegesuchen.\n",
    "\n",
    "## Abschluss\n",
    "\n",
    "Wir haben nur eine sehr grobe Bereinigung des Datensatzes durchgeführt. So haben wir unter anderem keine URLs entfernt, was zum Auftreten von typischen URL-Bestandteilen wie com, edu und gov geführt hat. Außerdem haben wir keine Auflösung von Verkürzungen vorgenommen, sodass auch Terme wie don (für don’t) und ve (für das verkürzte have) in unserer Themenliste auftauchen. Wenn wir uns einen kleinen Ausschnitt des erzeugten Wörterbuchs anzeigen lassen, wird auch schnell klar, dass eine Rechtschreibkorrektur und eine Lemmatisierung sicher ganz sinnvoll gewesen wären. Probier‘ doch selber einmal aus, das Ergebnis zu verbessern!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ecad0-40d9-4c52-8fb0-d5bed1f6df52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T11:59:33.602158Z",
     "start_time": "2025-03-17T11:59:33.598916Z"
    }
   },
   "outputs": [],
   "source": [
    "print(terms[977:996])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (AI4all_I)",
   "language": "python",
   "name": "python-ai4all_i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
