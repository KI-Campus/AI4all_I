{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94NZ-DC8ekBa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Installation und Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4owd0ZjOR7ke",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.data import Sentence\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A_DljvR8VyA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zusatzmaterial\n",
    "\n",
    "In diesem Abgschnitt gibt es einige Python-Methoden, die in der Haupterklärung nicht ausführlich behandelt wurden. Diese Methoden können jedoch nützlich sein, wenn Du tiefer in die Funktionsweise des Codes eintauchen möchtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brl9kLe_8bUP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_word_similarity(word_a, word_b, words, embeddings):\n",
    "    \"\"\"\n",
    "    Gibt die Kosinusähnlichkeit zwischen den Einbettungen zweier Wörter aus.\n",
    "    \n",
    "    Parameter:\n",
    "    wort_a (str): Das erste zu vergleichende Wort.\n",
    "    wort_b (str): Das zweite zu vergleichende Wort.\n",
    "    \n",
    "    Gibt zurück:\n",
    "    Keine\n",
    "    \n",
    "    Beispiel:\n",
    "    print_example_word_similarity(\"king\", \"man\")\n",
    "    # Ausgabe: Ähnlichkeit von König und Mann: 0.61\n",
    "    \"\"\"\n",
    "    embedding_a = [embeddings[words.index(word_a)]]\n",
    "    embedding_b = [embeddings[words.index(word_b)]]\n",
    "    print(\"Similarity of \", word_a, \" and \", word_b, \":\", *cosine_similarity(embedding_a, embedding_b).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_8_EnJ39Ils",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_suggestion(a, b, c, words, embeddings):\n",
    "    \"\"\"\n",
    "    Finde den Wortvorschlag für eine gegebene Gleichung \"a - b = x - c\".\n",
    "\n",
    "    Parameter:\n",
    "    a (str): Das erste Wort in der Gleichung.\n",
    "    b (str): Das zweite Wort in der Gleichung.\n",
    "    c (str): Das dritte Wort in der Gleichung.\n",
    "\n",
    "    words (list): Ein Liste mit Wörtern.\n",
    "    embeddings (list): Ein Liste mit Wort-Einbettungen.\n",
    "\n",
    "    Rückgabe:\n",
    "    str: Der Wortvorschlag für die Gleichung \"a - b = x - c\".\n",
    "    \"\"\"\n",
    "\n",
    "    lookup = dict(zip(words, embeddings))\n",
    "\n",
    "    # Ermittelt die Worteinbettungen für a, b, und c\n",
    "    vector_a = lookup.get(a)\n",
    "    vector_b = lookup.get(b)\n",
    "    vector_c = lookup.get(c)\n",
    "\n",
    "    # Erstellen eines Suchraums durch Entfernen von a, b und c aus dem Lookup-Dictionary\n",
    "    search_space = {\n",
    "        key: value for key, value in lookup.items()\n",
    "        if key not in [a, b, c]\n",
    "    }\n",
    "\n",
    "    # Berechnung der Einbettung für x durch Subtraktion von Vektor b von Vektor a und Addition von Vektor c\n",
    "    embedding_x = vector_a - vector_b + vector_c\n",
    "\n",
    "    # Erstellen Sie eine Liste aller Einbettungen im Suchraum.\n",
    "    embeddings = list(search_space.values())\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    # Ermittlung der Kosinusähnlichkeit zwischen Einbettung_x und allen Einbettungen im Suchraum\n",
    "    similarities = cosine_similarity(embeddings, embedding_x.reshape(1, -1))\n",
    "\n",
    "    # Ermittelt den Index der ähnlichsten Einbettung\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    # Ermittelt das Wort, das der ähnlichsten Einbettung entspricht.\n",
    "    words = list(search_space.keys())\n",
    "    most_similar_word = words[most_similar_index]\n",
    "\n",
    "    # Ausgeben der Wortgleichung\n",
    "    print(a + \" - \" + b + \" = \" + most_similar_word + \" - \" + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWVKmnaVBDfx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_word_embeddings(words, embeddings):\n",
    "    \"\"\"\n",
    "    Nimmt eine Liste von Wörtern und ihre entsprechenden Einbettungen und visualisiert sie im 3D-Raum mit PCA.\n",
    "    Die Visualisierung erfolgt über ein Streudiagramm, in dem jedes Wort durch einen Punkt im 3D-Raum dargestellt wird, \n",
    "    und Wörter der gleichen Kategorie werden durch die gleiche Farbe dargestellt.\n",
    "\n",
    "    Parameter:\n",
    "    words (Liste): Eine Liste von Wörtern.\n",
    "    embeddings (Liste): Eine Liste der entsprechenden Worteinbettungen.\n",
    "\n",
    "    Rückgabe:\n",
    "    Keine\n",
    "    \"\"\"\n",
    "    word_embeddings = dict(zip(words, embeddings))\n",
    "\n",
    "    pca = PCA(n_components=3, random_state=12345)\n",
    "    pca_embeddings = pca.fit_transform(list(word_embeddings.values()))\n",
    "    \n",
    "    df_pca = pd.DataFrame(pca_embeddings)\n",
    "    df_pca['word'] = word_embeddings.keys()\n",
    "    df_pca = df_pca.rename(columns={0: 'x', 1: 'y', 2: 'z'})\n",
    "    \n",
    "    labels = list()\n",
    "    for word in [\"royal\", \"sex\", \"digits\", \"numbers\", \"car\", \"aircraft\", \"cities\", \"nations\"]:\n",
    "        for i in range(10):\n",
    "            labels.append(word)\n",
    "    df_pca['category'] = labels\n",
    "    \n",
    "    fig = px.scatter_3d(df_pca, x=\"x\", y=\"y\", z=\"z\", text=\"word\", color=\"category\", opacity=0.9, template=\"plotly_dark\")\n",
    "    fig.update_traces(marker_size = 5)\n",
    "    fig.update_scenes(xaxis_visible=False, yaxis_visible=False,zaxis_visible=False)\n",
    "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        fig.show(renderer='colab') \n",
    "    else:\n",
    "        fig.show(renderer='iframe') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHv5TOefe8O6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Text und Computer\n",
    "\n",
    "Wenn wir Texte verarbeiten, denken wir selten darüber nach, wie wir sie in einer für Computer verständlichen Form darstellen können. Text-Repräsentation und Wort-Einbettungen sind jedoch entscheidende Konzepte in der natürlichen Sprachverarbeitung, die uns dabei helfen, genau das zu tun!\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) ist ein fortschrittlicheres Modell zur Wort-Einbettung, das die Bedeutung von Wörtern anhand ihres Kontexts erfasst und sie in einem mehrdimensionalen Raum positioniert. Das mag sich zwar kompliziert anhören, aber es ist eigentlich ziemlich cool. Denn im Gegensatz zu One-Hot-Encodings kann BERT die Bedeutungen von Worten und deren Kontext besser darstellen. Das führt zu einer höheren Genauigkeit in der Sprachverarbeitung und hilft uns, noch besser zu verstehen, was in Texten wirklich vor sich geht.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBBEbr-FRc1R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere BERT als Basis-Modell\n",
    "transformer = TransformerWordEmbeddings('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXsC5TVyV1RT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufbau von BERT\n",
    "\n",
    "Eine weitere faszinierende Eigenschaft von BERT ist, dass es bereits vortrainiert ist. Das bedeutet, dass es mit einer großen Menge an Texten trainiert wurde, um eine allgemeine Sprachkompetenz zu erwerben. Das ist vergleichbar mit dem Erlernen einer Sprache durch einen Menschen, der eine Vielzahl von Texten liest und somit ein besseres Verständnis für die Sprache entwickelt.\n",
    "\n",
    "Dabei erlernt BERT ein Vokabular aus etwa 30.000 Wörtern und Subworten. Das Modell verwendet das WordPiece-Vokabular, das bedeutet, dass lange Wörter in kleinere, häufig verwendete Teile zerlegt werden, die als Subwörter bezeichnet sind. Jedes Wort oder Subwort im Vokabular wird durch einen eindeutigen Vektor dargestellt, der im Modell verwendet wird, um die Bedeutung des Wortes zu erfassen.\n",
    "\n",
    "Ein Beispiel für ein solches Wort im WordPiece-Vokabular von BERT ist \"doghouse\". Da BERT mit einem unverarbeiteten Text trainiert wurde, wird das Wort in kleinere Subwörter aufgeteilt. Im Vokabular von BERT wird \"doghouse\" in zwei Subwörter unterteilt: \"dog\", und \"##house\" wobei das Symbol \"##\" anzeigt, dass das Subwort ein Teil eines längeren Wortes ist.\n",
    "\n",
    "Jedes dieser Subwörter hat eine eindeutige ID im Vokabular und wird durch einen entsprechenden Vektor repräsentiert. Wenn BERT mit einem Text arbeitet, der das Wort \"doghouse\" enthält, wird das Modell jedes Subwort erkennen und deren Vektoren kombinieren, um eine umfassende Repräsentation des Wortes zu erstellen.\n",
    "\n",
    "Zusätzlich erinnert die Architektur von BERT an den Aufbau von Neuronalen-Netzen. Es besteht aus vielen Schichten von sogenannten Encodern, die Informationen von vorherigen Schichten aufnehmen und verarbeiten, um kontextabhängige Repräsentationen von Wörtern zu erzeugen. Diese Architektur hat sich als sehr erfolgreich erwiesen und ist ein weiterer Beweis dafür, wie mächtig Neuronale-Netze in der heutigen Zeit sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QtqqZ32YY2Lo",
    "outputId": "971e1f40-58d0-4bef-9d45-022b75597347",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bert_vocabulary = transformer.tokenizer.vocab\n",
    "\n",
    "print(\"Größe Vokabular:\" , len(bert_vocabulary))\n",
    "print(\"Größe Einbettung:\", transformer.model.embeddings.word_embeddings)\n",
    "\n",
    "print(\"Id für das Wort 'dog':\", bert_vocabulary.get('dog'))\n",
    "print(\"Id für das Wort '##house':\", bert_vocabulary.get('##house'))\n",
    "print(\"Id für das Wort 'house':\", bert_vocabulary.get('house'))\n",
    "print(\"Id für das Wort 'doghouse':\", bert_vocabulary.get('doghouse'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBI_2nm8SDew",
    "outputId": "98798d8f-49af-4ca3-c3a4-62813a9cc05b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Erhalte die Schichten des Transformers\n",
    "transformer_layers = transformer.model.encoder.layer\n",
    "print(\"Transformer Schichten:\", len(transformer_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sP0AaA3IdNzg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# King - Man = X - Woman? Ein Beispiel für Textverarbeitung\n",
    "\n",
    "Ein klassisches Beispiel um das Konzept der Wort-Einbettungen und die Vorteile der BERT-Einbettungen zu demonstrieren ist das Rätsel: \"king - man = x - woman\". Dabei werden die Vektoren der Wörter \"king\", \"man\", \"woman\" und \"queen\" sinnbildlich in Bezug zueinander gestellt, um das Wort \"x\" zu erhalten. Die Intuition dahinter ist, dass die Differenz zwischen \"king\" und \"man\" die Eigenschaften repräsentiert, die einzigartig für das Wort \"king\" sind, und das Hinzufügen der Eigenschaften von \"woman\" zu dieser Differenz die Eigenschaften ergibt, die einzigartig für das Wort \"queen\" sind.\n",
    "\n",
    "Der Vorteil in der Verwendung von BERT-Einbettungen besteht darin, dass sie auf einem starken Sprachmodell basieren, und so in der Lage sind, anspruchsvollere und nuanciertere Beziehungen (Semantik) zwischen Wörtern erfassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVp-5pZufZao",
    "outputId": "3a22fbfa-36d8-4ab4-ad12-17c72ea234b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier siehst du einen Text aus jeweils 10 Worten für 8 Klassen, welche die 4 Kategorien Menschen, Zahlen, Fahrzeuge und Länder beschreiben.\n",
    "royal = \"royal monarchy crown queen king coronation throne palace majestic imperial\"\n",
    "sex = \"sex man woman diversity transgender masculinity femininity intersex LGBTQ gender\"\n",
    "\n",
    "digits = \"1 2 3 4 5 6 7 8 9 10\"\n",
    "numbers = \"one two three four five six seven eight nine ten\"\n",
    "\n",
    "car = \"car wheels engine speed road drive auto fuel vehicle emission\"\n",
    "aircraft = \"aircraft airbus boeing B747 A380 pilot cockpit wings takeoff landing\"\n",
    "\n",
    "cities = \"Beijing Tokyo London Paris Berlin Rome Madrid Warsaw Bangkok Bern\"\n",
    "nations = \"China Japan UK France Germany Italy Spain Poland Thailand Switzerland\"\n",
    "\n",
    "corpus = \" \".join([royal, sex, digits, numbers, car, aircraft, cities, nations])\n",
    "\n",
    "print(\"Text of all words:\", corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_vyR5KifmYa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Vom Wort zum Vektor\n",
    "\n",
    "Das Sentence-Objekt in Flair hilft dabei, Texte und Sätze als Vektoren darzustellen. Das ist hilfreich, um Texte zu analysieren und verschiedene Aufgaben in der Sprachverarbeitung zu lösen.\n",
    "\n",
    "Um Vektoren mit dem Sentence-Objekt zu erstellen, wird einfach die .embed() Methode aufgerufen. Dabei werden vorab trainierter BERT-Embedding-Vektoren verwendet, die dann für die jeweiligen Worte zusammengesetzt werden. Das ist ähnlich wie der Aufruf von fit_transform() in der sklearn Bibliothek, wo ein vorab trainiertes Modell die gegebenen Daten in eine andere Darstellung transformiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zNS6dKNvfecP",
    "outputId": "fa461371-aca9-4224-e6e2-48db51c0f06a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier wird aus unserem Korpus ein Satz aus einzelnen Worten gebildet.\n",
    "sentence = Sentence(corpus)\n",
    "\n",
    "# Hier wird für jedes einzelne Wort eine Einbettung erzeugen.\n",
    "transformer.embed(sentence)\n",
    "\n",
    "# Hier werden für die Wörter und Embeddings separate Listen erstellt.\n",
    "words = list()\n",
    "embeddings = list()\n",
    "\n",
    "for token in sentence:\n",
    "    word = token.text\n",
    "    embedding = token.embedding.numpy()\n",
    "    words.append(word)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(\"Beispiel Wort:\", words[0])\n",
    "print(\"Einbettung Wort:\", embeddings[0][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IEKEXjm6DX9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Semantische Ähnlichkeit von Wörtern\n",
    "\n",
    "Semantische Ähnlichkeit ist eine Methode zur Messung der Ähnlichkeit zwischen Bedeutungen von Wörtern. Es hilft uns, in Anwendungen der Textverarbeitung besser zu machen, indem es uns ermöglicht, die Bedeutung von Wörtern zu verstehen und darzustellen.\n",
    "\n",
    "Daher kännen wir dieses Konzept verwenden, um die Gleichung \"king - man = c - woman\" mit BERT-Einbettungen zu lösen, indem die Ähnlichkeiten zwischen den Wörtern in der Gleichung verglichen werden.\n",
    "\n",
    "In diesem Gleichnis geht es darum, das Wort \"x\" zu finden, das dem Wort \"king\" am ähnlichsten ist, während es dem Wort \"man\" am unähnlichsten ist. In ähnlicher Weise sollte das Wort \"woman\" dem Wort \"man\" am ähnlichsten sein.\n",
    "\n",
    "Um das Wort \"x\" zu finden, können wir schlichtweg die Kosinus-Ähnlichkeit zwischen den BERT-Einbettungen der einzelnen Wortpaare in der Gleichung berechnen.\n",
    "Diese ist für eine solche Aufgabe gut geeignet, da sie den Kosinus des Winkels zwischen zwei Vektoren in einem hochdimensionalen Raum misst, wobei jeder Vektor die Einbettung eines Wortes darstellt.\n",
    "\n",
    "Die Kosinus-Ähnlichkeitsmetrik reicht von 0 bis 1, wobei 1 bedeutet, dass zwei Wörter genau dieselbe Bedeutung haben, und 0 bedeutet, dass die beiden Wörter völlig unterschiedliche Bedeutungen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vl0ayx247iO6",
    "outputId": "3b10bfad-bdf1-4250-9084-ad7669fd3bce",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier siehst Du, dass Worte ähnlich oder unähnlich sein können.\n",
    "print_word_similarity(\"king\", \"queen\", words, embeddings)\n",
    "print_word_similarity(\"man\", \"queen\", words, embeddings)\n",
    "print_word_similarity(\"woman\", \"queen\", words, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-XwuY7C8_b9",
    "outputId": "7bef2903-4ff0-460f-b381-47c0beb4366d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier siehst du, dass die Wort-Einbettungen einer gewissen Logik folgen.\n",
    "get_word_suggestion(\"king\", \"man\", \"woman\", words, embeddings)\n",
    "get_word_suggestion(\"3\", \"1\", \"2\", words, embeddings)\n",
    "get_word_suggestion(\"three\", \"one\", \"two\", words, embeddings)\n",
    "get_word_suggestion(\"aircraft\", \"wings\", \"wheels\", words, embeddings)\n",
    "get_word_suggestion(\"Germany\", \"Berlin\", \"Bangkok\", words, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "WWDMLy0ZAHZQ",
    "outputId": "b278ca75-45bb-4d7d-8d51-7b7fcc1a920d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier kannst du einen interaktiven Plot der Wöter und deren Bedeutung im 3-Dimensionalen betrachten.\n",
    "visualize_word_embeddings(words, embeddings)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "94NZ-DC8ekBa"
   ],
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (AI4all_I)",
   "language": "python",
   "name": "python-ai4all_i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
