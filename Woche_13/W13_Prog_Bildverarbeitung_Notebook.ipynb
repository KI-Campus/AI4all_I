{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQRaNxTSwvRe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Installation und Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4UyZlZYujU7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2Pi7UEi1GaF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Zusatzmaterial\n",
    "In diesem Abgschnitt gibt es einige Python-Methoden, die in der Haupterklärung nicht ausführlich behandelt wurden. Diese Methoden können jedoch nützlich sein, wenn Du tiefer in die Funktionsweise des Codes eintauchen möchtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0FvtIsbZ1KDb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def show_example(X, y, index):\n",
    "    \"\"\"\n",
    "    Zeigt ein Bild aus einer Sammlung von Bildern zusammen mit seiner Beschriftung und Pixelstatistik an.\n",
    "    \n",
    "    Args:\n",
    "    - X: Ein numpy-Array der Form (n, Höhe, Breite), das die Bilder darstellt.\n",
    "    - y: Ein numpy array of shape (n, ), das die entsprechenden Labels darstellt.\n",
    "    - index: Eine ganze Zahl, die den Index des anzuzeigenden Bildes angibt.\n",
    "    \n",
    "    Rückgabe:\n",
    "    - Keine\n",
    "    \n",
    "    Nebeneffekte:\n",
    "    - Druckt die Form des Bildes.\n",
    "    - Druckt die Bezeichnung des Bildes.\n",
    "    - Gibt den Wert des mittleren Pixels des Bildes aus.\n",
    "    - Druckt die minimalen und maximalen Pixelwerte des Bildes.\n",
    "    - Zeigt das Bild mit Hilfe der Plotly Express-Bibliothek an.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Da die Label in CIFAR10 sind numerisch, beschreiben aber die Objekte in dieser Liste\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    image = X[index]\n",
    "    label = y[index]\n",
    "\n",
    "    if type(label) == np.ndarray:\n",
    "        label = label[0] # this is because the label itself might be packed into a list.\n",
    "        label = y[index][0]\n",
    "\n",
    "    print(label)\n",
    "\n",
    "    print(\"Struktur vom Bild:\", image.shape) # The shape can be read (x, y, pixel-values)\n",
    "    print(\"Label:\", class_names[label])\n",
    "    print(\"Pixel in der Mitte:\", image[int(image.shape[0]/2), int(image.shape[1]/2)])\n",
    "    print(\"Farbwerte (dunkel -> hell):\", image.min(), \"to\", image.max())\n",
    "    fig = px.imshow(image, binary_string=True)\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        fig.show(renderer='colab') \n",
    "    else:\n",
    "        fig.show(renderer='iframe') \n",
    "    return np.array([image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJuC_anz1oG1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plotten Sie den Trainings- und Validierungsverlust und die Genauigkeit mit Matplotlib und Plotly.\n",
    "\n",
    "    Args:\n",
    "    - history: Ein Keras History-Objekt, das von der Methode \"fit()\" zurückgegeben wird.\n",
    "    \n",
    "    Rückgabe:\n",
    "    - Nichts.\n",
    "    \"\"\"\n",
    "    # Trainings- und Validierungsverluste\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Genauigkeit\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    # Anzahl der Epochen\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    # Alternative: interaktive Plotly-Diagramme\n",
    "    fig_loss = px.line(history.history, y=['loss', 'val_loss'],\n",
    "                       labels={'index': 'Epoch', 'value': 'Loss', 'loss': 'Training loss'}, \n",
    "                       title='Training and validation loss')\n",
    "\n",
    "    fig_acc = px.line(history.history, y=['accuracy', 'val_accuracy'], labels={'index': 'Epoch', 'value': 'Accuracy'}, \n",
    "                      title='Training and validation accuracy')\n",
    "\n",
    "    new_names_loss = {'loss':'Training loss', 'val_loss': 'Validation loss'}\n",
    "    fig_loss.for_each_trace(lambda t: t.update(name = new_names_loss[t.name],\n",
    "                                          legendgroup = new_names_loss[t.name],\n",
    "                                          hovertemplate = t.hovertemplate.replace(t.name, new_names_loss[t.name])\n",
    "                                         )\n",
    "                      )\n",
    "\n",
    "    new_names_acc = {'accuracy':'Training accuracy', 'val_accuracy': 'Validation accuracy'}\n",
    "    fig_acc.for_each_trace(lambda t: t.update(name = new_names_acc[t.name],\n",
    "                                          legendgroup = new_names_acc[t.name],\n",
    "                                          hovertemplate = t.hovertemplate.replace(t.name, new_names_acc[t.name])\n",
    "                                         )\n",
    "                      )\n",
    "\n",
    "    fig_loss.show()\n",
    "    fig_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL0V6afRxD_L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Der CIFAR10 Datensatz\n",
    "\n",
    "Der [CIFAR-10](!https://www.cs.toronto.edu/~kriz/cifar.html) Datensatz ist ein umfangreicher Datensatz von 60.000 Farbbildern, die in 10 Klassen unterteilt sind. Jede Klasse umfasst 6.000 Bilder von Objekten, die in der realen Welt vorkommen, wie Flugzeuge, Autos, Tiere und Schiffe.\n",
    "\n",
    "Ein Unterschied zwischen dem CIFAR-10 Datensatz und dem MNIST Datensatz besteht darin, dass CIFAR-10 Farbbilder unterschiedlicher Objekte, während MNIST Graustufenbilder von handgeschriebenen Ziffern enthält. Ein weiterer Unterschied ist, dass die Bilder im CIFAR-10 Datensatz 32x32 Pixel groß sind, während die Bilder im MNIST Datensatz 28x28 Pixel haben.\n",
    "\n",
    "Diese Unterschiede machen den CIFAR-10 Datensatz zu einer größeren und komplexeren Herausforderung für neuronale Netze, während der MNIST Datensatz eher als Einstiegspunkt für das Training von Bilderkennungsalgorithmen verwendet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZ5wqMDezMfX",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Zuerst werden die Daten geladen.\n",
    "(x_train, y_train), (x_val, y_val) = datasets.cifar10.load_data()\n",
    "# Da CIFAR10 keine finalen Test-Daten hat, kannst du auch einfach die letzen 10.000 Trainingsdaten dazu verwenden.\n",
    "x_train, x_test = x_train[:40000], x_train[40000:]\n",
    "y_train, y_test = y_train[:40000], y_train[40000:]\n",
    "\n",
    "# Normalisierung der Farbbilder.\n",
    "x_train, x_val, x_test = x_train / 255.0, x_val / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tu2ReCZxzTad",
    "outputId": "2fdbbfea-4e5a-47b0-ed47-31aaca37c0e1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Mit .shape kannst Du sehen wie der Aufbau der Daten ist.\n",
    "# Du kannst sehen, dass der erste Eintrag die Gesamtzahl der Daten angibt.\n",
    "# Die darauf folgenden drei Werte hingegen geben die Größe (Höhe, Breite) sowie die Farbkanäle RBG der Bilder an.\n",
    "print(\"Trainings Daten:\", x_train.shape)\n",
    "print(\"Validation Daten:\", x_val.shape)\n",
    "print(\"Test Daten:\", x_test.shape)\n",
    "# Bei den Labeln ist es anders als bei den Bilddaten, hier haben wir nur einzelne Werte.\n",
    "print(\"Trainings Label:\", y_train.shape)\n",
    "print(\"Validation Label:\", y_val.shape)\n",
    "print(\"Test Label:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvvdrpPf0XHA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aufbau von Farbbildern\n",
    "\n",
    "Der CIFAR-10 Datensatz enthält Farbbilder, die in der Regel eine Größe von 32x32 Pixeln haben. Jedes Bild wird durch drei Farbkanäle dargestellt: Rot, Grün und Blau (RGB). Jeder Kanal wird als eine Matrix von 32x32 Werten repräsentiert, wobei jeder Wert die Intensität der entsprechenden Farbe an einer bestimmten Position im Bild angibt.\n",
    "\n",
    "Auch bei noralisierten Farbbildern reichen die Werte für jeden Kanal von 0 bis 1, wobei 0 für das Fehlen der Farbe und 1 für maximale Intensität steht. Durch Kombination der drei Kanäle können die Farben jedes Pixels im Bild dargestellt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "-ScVxcdU0-Eu",
    "outputId": "dd97eb66-ab45-41a8-97fa-36c89f980251",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier kannst du interaktiv ein Beispiel betrachten.\n",
    "example = show_example(x_train, y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VqlKtCShRLd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Aufbau eines CNN\n",
    "Convolutional Neural Networks (CNNs) sind eine Art von künstlichen neuronalen Netzwerken, die besonders gut für die Verarbeitung von Bildern geeignet sind.\n",
    "\n",
    "Ein CNN besteht aus verschiedenen Schichten, die nacheinander auf das Bild angewendet werden. Die ersten Schichten eines CNNs sind oft Convolutional Layers. Diese Schichten suchen im Bild nach bestimmten Mustern, wie zum Beispiel Ecken, Kanten oder Formen, und speichern diese Merkmale als Ergebnis ab.\n",
    "\n",
    "## Convolutional Schichten\n",
    "\n",
    "Im Code, den Du siehst, wird in der ersten Conv2D-Schicht das Eingabebild durch 32 sogenannte Filter bearbeitet. Ein Filter ist wie eine kleine Schablone, die auf das Bild gelegt wird und bestimmte Muster sucht. Ein Filter kann zum Beispiel nach vertikalen oder horizontalen Kanten suchen oder nach anderen spezifischen Mustern.\n",
    "\n",
    "Die Größe des Filters in diesem Code beträgt 3x3 Pixel. Die Conv2D-Schicht geht also Pixelweise durch das Bild und versucht, in jedem Ausschnitt von 3x3 Pixeln das Muster zu finden. Dabei wird für jeden Ausschnitt ein neues Ergebnis berechnet, das ein Merkmal des Bildes darstellt.\n",
    "\n",
    "Nach der ersten Schicht kommt eine MaxPooling2D-Schicht. Diese Schicht verkleinert das Bild, indem sie die Ergebnisse der vorherigen Schicht zusammenfasst. Dabei wird das Bild in Quadrate von 2x2 Pixeln aufgeteilt und für jedes Quadrat der höchste Wert genommen. Durch diese Zusammenfassung wird das Bild vereinfacht und es werden unwichtige Details entfernt.\n",
    "\n",
    "In der zweiten Schicht wird das Bild wieder durch Filter geschickt, die auf die Merkmale der ersten Schicht aufbauen. Es werden diesmal 64 Filter verwendet, um noch spezifischere Merkmale zu finden.\n",
    "\n",
    "## Fully-Connected Schichten\n",
    "\n",
    "Nach den Convolutional-Schichten folgen in der Regel Fully-Connected Schichten, die die Merkmale klassifizieren und zur Ausgabe führen. In diesem Zusammenhand bedeutet Fully-Connected, dass jedes Neuron in einer Schicht mit jedem Neuron in der darauffolgenden Schicht verbunden ist, also Informationen an jedes Neuron der nächsten Schicht weitergibt.\n",
    "\n",
    "In diesem Code wird eine Flatten-Schicht verwendet, um die Ausgabe der vorherigen Schicht in einen Vektor umzuwandeln. Das kannst Du damit vergleichen, dass das Bild, wie bei einem Aktenvernichter, zeilenweise zerschnitten und der Länge nach wieder zusammengesetzt wird.\n",
    "Dieser Vektor wird dann durch zwei Dense-Schichten weiterverarbeitet und in die letzte Dense-Schicht hat 10 Neuronen weitergereicht. Die Softmax-Aktivierungsfunktion wird verwendet, um die Wahrscheinlichkeiten der Vorhersage der 10 möglichen Klassen zu normalisieren und die Ergebnisse interpretierbar zu gestalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA4zIIPDhcwF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "# Die Convolutional Schichten\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Die Fully-Connected Schichten\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xjjkdZy7huac",
    "outputId": "ae709482-aaca-40f9-c0c2-4b3bba0534f3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKMWTIEMlImz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interpretation\n",
    "\n",
    "Wenn Du Dir das Modell ansiehst, wirst Du vielleicht verwirrt sein, weil die Dimensionalität der Eingabedaten für jede Schicht abnimmt. Ebenso könnte es aber auch gut sein, dass Du nicht ganz verstehst, wie eine Convolutional Schicht ihre Parameter erzeugt. \n",
    "Daher hier eine kurze Erklärung um das CNNs besser zu interpretieren.\n",
    "\n",
    "## Paramters einer Convolutional Schicht\n",
    "Betrachte zu Beginn die erste Conv2D Schicht mit 896 Parametern.\n",
    "Die Anzahl der Parameter in dieser Schicht hängt von unterschiedlichen Faktoren ab, wie der Größe der Filter, der Anzahl der Filter und der Größe des Eingabebildes.\n",
    "\n",
    "Das abgebiltete CNN, erzeugt für die erste Conv2D Schicht 32 Filter mit einer Größe von 3x3 Pixeln und eine Eingabeform von 32x32x3 Pixeln (3 für die Farbkanäle RGB). Jeder Filter hat also eine Größe von 3x3x3 = 27 Elemente. Da jeder Filter mit jedem Ausschnitt im Bild getestet werden muss, ergibt das insgesamt 32 x 27 = 864 Parameter.\n",
    "\n",
    "Zusätzlich gibt es noch einen Bias-Wert für jeden Filter, der die Aktivierung der Filter steuert. Daher haben wir insgesamt 864 + 32 = 896 Parameter für die erste Conv2D Schicht.\n",
    "\n",
    "## Veränderung Dimensionalität\n",
    "Die Dimensionalität der Eingabe verändert sich im CNN, weil es Filter auf die Eingabe anwendet, welche die Größe und damit die Dimensionalität der Eingabe verändern können.\n",
    "\n",
    "In diesem CNN wird eine Eingabe von 32x32x3 Pixeln (Breite x Höhe x Farbkanäle) verwendet. In der ersten Conv2D Schicht werden 32 Filter mit einer Größe von 3x3 Pixeln auf die Eingabe angesetzt. Dies führt zu 32 Ausgaben (auch Feature Maps genannt), die jeweils eine Größe von 30x30 Pixeln haben. Der Grund dafür ist, dass die Filter die Ränder des Eingabebildes nicht überschreiten können und somit eine Ausgabe, die kleiner ist als die Eingabe, erzeugen.\n",
    "\n",
    "Anschließend wird eine MaxPooling2D Schicht mit einem 2x2 Pixel großen Fenster verwendet, um die Dimensionalität der Ausgabe zu reduzieren. Diese Schicht extrahiert den größten Wert aus jedem 2x2 Pixelbereich der Eingabe und gibt eine Ausgabe zurück, die halb so groß ist wie die Eingabe. In diesem Fall wird die Ausgabe von 30x30 auf 15x15 Pixel verkleinert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5cNdcyOqyt2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Trainieren und Validieren des CNN\n",
    "\n",
    "In diesem Codeabschnitt siehst Du, wie das CNN trainiert wird, um Bilder zu klassifizieren. Anfänglich wurde der Datensatz in drei Teile aufgeteilt: Trainings-, Validierungs- und Testdaten.\n",
    "\n",
    "In diesem Abschnitt kommen die drei Teile des Datensatzes tatsächlich zum Einsatz, doch vorher ein paar Worte noch:\n",
    "\n",
    "Die Trainingsdaten werden verwendet, um das Modell zu trainieren. Das Modell nutzt diese Daten, um seine Gewichte und Parameter anzupassen, um eine möglichst gute Leistung auf den Trainingsdaten zu erzielen.\n",
    "\n",
    "Die Validierungsdaten werden verwendet, um die Leistung des Modells zu bewerten, während es trainiert wird. Das Modell nutzt diese Daten, um die Hyperparameter und die Architektur des Modells zu optimieren und eine Überanpassung an die Trainingsdaten zu vermeiden. Die Validierungsdaten helfen dabei, die beste Modellkonfiguration auszuwählen, die die bestmögliche Leistung auf unbekannten Daten erzielt.\n",
    "\n",
    "Die Testdaten werden verwendet, um die endgültige Leistung des Modells auf unbekannten Daten zu bewerten. Diese Daten wurden während des Trainings und der Validierung nicht verwendet und dienen daher als objektiver Maßstab für die Leistung des Modells.\n",
    "\n",
    "Durch die Verwendung von Trainings-, Validierungs- und Testdaten können wir sicherstellen, dass das Modell nicht nur auf den Trainingsdaten, sondern auch auf unbekannten Daten gut abschneidet. Es ist wichtig, dass diese drei Datensätze getrennt und unabhängig voneinander gehalten werden, um eine objektive Bewertung des Modells zu gewährleisten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpujhR4qq2yW",
    "outputId": "e415f9df-4071-435c-b913-aebc971fa875",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Aus Woche 11 bekannt.\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# Aus Woche 11 bekannt.\n",
    "history = model.fit(x_train, y_train, epochs=5,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qM_yoh4-xpmm",
    "outputId": "fe192580-f54c-4e49-9d74-bc580b87b57b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0eMHMZM3Y2A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testen des CNN auf unbekannten Daten\n",
    "\n",
    "Das Testen eines CNN auf unbekannten Daten ist ein aufregender Moment, denn es gibt Dir einen Einblick in die tatsächliche Leistungsfähigkeit des Modells. Nach dem Training und der Validierung ist es Zeit, das Modell zu testen, indem Du es auf einen Satz von Daten anwendest, die das CNN noch nie zuvor gesehen hat.\n",
    "\n",
    "Die Idee dahinter ist, herauszufinden, ob das Modell in der Lage ist, gut zu generalisieren - also, ob es in der Lage ist, nützliche Muster in Bildern zu erkennen, die es zuvor noch nicht gesehen hat. Wenn das Modell eine hohe Genauigkeit auf diesen unbekannten Daten aufweist, dann kannst Du ziemlich zuversichtlich sein, dass das Modell gut funktioniert und, ob es tatsächlich in der Lage ist, die gewünschten Muster in den Bildern zu erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsoukN4Mr_oG",
    "outputId": "fa24cac7-bed3-4881-906a-ce32edb9c315",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "# Du siehst, dass das CNN mit gut 68% recht akkurate Vorhersagen treffen kann.\n",
    "print(\"Test accuracy:\", round(test_acc, 4)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUqpO2Fb41tM",
    "outputId": "7bcc1127-8451-4993-afc0-55e669aa8daf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Du kannst mit dem CNN auch direkt Vorhersagen treffen.\n",
    "predictions = model.predict(x_test)\n",
    "y_predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g06674lL5VFY",
    "outputId": "33426e86-1477-4c99-8030-b78004175b54",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hier wird ein Auto erkannt.\n",
    "example_prediction_correct = show_example(x_test, y_predictions, 0)\n",
    "# Hier wird ein Pferd nicht erkannt.\n",
    "example_prediction_incorrect = show_example(x_test, y_predictions, 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZQRaNxTSwvRe",
    "jL0V6afRxD_L",
    "9VqlKtCShRLd",
    "a5cNdcyOqyt2",
    "v0eMHMZM3Y2A"
   ],
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (AI4all_I)",
   "language": "python",
   "name": "python-ai4all_i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
